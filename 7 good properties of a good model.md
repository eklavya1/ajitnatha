---
alias: [2022-05-07,23:53,,,,,,,,,,,]
---
[[MIT open course introduction to psychology]]
table of content
```toc
```

[[2022-05-07]] 23:53 ["The Seven Properties of Good Models" | David Laibson](https://scholar.harvard.edu/laibson/publications/seven-properties-good-models)
the seven properties of good models1
xavier gabaix
mit and nber
and
david laibson
harvard university and nber
nyu methodology conference
 
1 the authors wish to thank andrew caplin for helpful advice. ian dew-becker and brian wells provided 
valuable research assistance. the authors acknowledge financial support from the national science 
foundation (hsd 0527518) and the national institute of aging (p01 ag005842).
scientists spend most of their time formulating and analyzing models. a model is 
a description – or representation – of the world. most models are based on assumptions 
that are known to be only approximately true (and exactly false). for example, consider 
the most commonly used models of the earth: flat, spherical, and ellipsoid. these models 
do not account for the bumps and grooves. a perfect replica of the earth would reproduce 
every contour, but such a representation would be impractical. you don’t need to know 
the height of beacon hill to take a subway across boston. tourists use a flat subway map 
-- the model that is just complex enough for the problem at hand.
this essay describes the seven key properties of useful economic models: 
parsimony, tractability, conceptual insightfulness, generalizability, falsifiability, 
empirical consistency, and predictive precision.2
successful economic models have most 
of these properties, although almost no economic models have them all. some of these 
seven properties are already well accepted among economists, specifically, parsimony, 
tractability, conceptual insightfulness, and generalizability. the other properties --
falsifiability, empirical consistency, and predictive precision – are not universally 
accepted.
we believe that these seven properties are fundamental. some economists instead 
argue that classical optimization assumptions -- like rationality and dynamic consistency
-- are necessary ingredients of a good economic model. we believe that these 
optimization assumptions do discipline economic analysis and often produce many of the 
 
2 many other authors have attempted to characterize the purposes of models. our formulation has been 
directly and indirectly influenced by preceding analyses by popper [1935, 1963], merton [1949], friedman 
[1953], kuhn [1962], and stigler [1965]. for a related contemporary analysis, see jasso [2004]. stigler 
[1965] is the most important precedent for the analysis in the current paper. stigler identifies three 
characteristics of economic theories that are accepted by ―leading economists.‖ stigler’s criteria are 
generality, manageability, and congruence with reality. 
seven properties listed above. for instance, parsimony, tractability, conceptual 
insightfulness, and generalizability all tend to follow from classical optimization
assumptions. however, such classical assumptions are not the only path to a good model. 
we believe that classical optimization assumptions are better treated as hypotheses that 
should be tested and not fundamental or necessary properties of economic models.
anticipating one set of objections, we also discuss how to conduct normative analysis 
even if economists can’t rely on optimization principles like revealed preferences.
we conclude the essay by discussing the appropriate empirical scope of economic 
models. we argue that restrictions on the scope of economic research contradict the 
history of science, as well as the conceptual orientation of economics itself. 
seven key properties of economic models.
economic models are conceptual frameworks that aid in the understanding, 
description, and/or prediction of human behavior. formal empirical analysis, casual 
observation, and introspection all play a role in demonstrating the correspondences 
between economic models and the world.
in this section, we define the seven key properties of good models. we note that 
these properties are not always mutually consistent. consequently, these properties 
sometimes need to be traded off against each other. 
1. parsimony
2. tractability
3. conceptual insightfulness
4. generalizability
5. falsifiability
6. empirical consistency
7. predictive precision
parsimonious models are simple models in the sense that they rely on relatively 
few special assumptions and they leave the researcher with relatively few degrees of 
freedom.3
parsimonious models are desirable because they prevent the researcher from 
consciously or subconsciously manipulating the model so that it over-fits the available 
facts. over-fitting occurs when a model works very well in a given situation, but fails to 
make accurate out-of-sample predictions. for example, if a model incorporates a large set 
of qualitative psychological biases then the model is nonparsimonious, since selective 
combination of those biases will enable the researcher to tweak the model so that it 
―explains‖ almost any pattern of observations. likewise, if a model has many free
parameters – for instance, a complex budget constraint or complex household preferences 
-- then the model is relatively nonparsimonious. when models are flexible and complex 
the researcher can combine the myriad elements to match almost any given set of facts.
such flexibility makes it easy to explain in-sample data, producing the false impression 
that the model will have real (out-of-sample) explanatory power (see figure 1).
 
3
these ideas are reflected in occam’s razor, a principle attributed to the logician william of ockham: ―all 
things being equal, the simplest solution tends to be the best one.‖
-2
2
6
0
figure 1: the value of parsimony.
the data (squares) is generated by sin(x/10) + ε, where ε is distributed 
uniformly between -½ and ½. the sold line fits the first 50 data points to 
a fifth-order polynomial – a non-parsimonious model. the polynomial 
has good fit in sample and poor fit out of sample (dashed line).
sample for estimation
of a 5th order polynomial
tractable models are easy to analyze. models with maximal tractability can be
solved with analytic methods – i.e. paper and pencil calculations. at the other extreme, 
minimally tractable models cannot be solved even with a computer, since the necessary 
computations/simulations would take too long. for instance, optimization is typically not 
computationally feasible when there are dozens of continuous state variables – in such 
cases, numerical solution times are measured on the scale of years or centuries.
conceptually insightful models reveal fundamental properties of economic 
behavior or economic systems. for example, the model of concave utility identifies the 
key property of risk aversion. the concept of concave utility is useful even though it 
makes only qualitative predictions. an optimizing framework like nash equilibrium is 
also conceptually insightful even though it relies on an assumption that is empirically 
false – perfect rationality. the concept of nash equilibrium clarifies some abstract ideas 
about equilibrium that are important to understand even if the nash framework is an 
incomplete explanation of real-world behavior. finally, many models are conceptually 
useful because they provide normative insights.
generalizable models can be applied to relatively wide range of situations. for 
example, a generalizable model of risk aversion could be used to analyze risk aversion in 
settings with small or large stakes, as well as risk aversion with respect to losses or gains.
a generalizable model of learning could be used to study learning dynamics in settings
with a discrete action set or a continuous action set or an action set with a mixture of 
discrete and continuous actions. a generalizable model of intertemporal choice could be 
used to study decisions with consequences that occur in minutes or decades.
falsifiability and prediction are the same concept. a model is falsifiable if and 
only if the model makes nontrivial predictions that can in principle be empirically 
falsified. if a model makes no falsifiable predictions, then the model can not be 
empirically evaluated.
empirically consistent models are broadly consistent with the available data. in 
other words, empirically consistent models have not yet generated predictions that have
been falsified by the data. empirically consistent models can be ranked by the strength of 
their predictions. at one extreme, a model can be consistent with the data if the model 
makes only weak predictions that are verified empirically. at the other extreme, models 
can achieve empirical consistency by making many strong – i.e. precise -- predictions 
that are verified empirically.
models have predictive precision when they make precise – or ―strong‖ –
predictions. strong predictions are desirable because they facilitate model evaluation and 
model testing. when an incorrect model makes strong predictions it is easy to 
empirically falsify the model, even when the researcher only has access to a small 
amount of data. a model with predictive precision also has greater potential to be 
practically useful if it survives empirical testing. models with predictive precision are 
useful tools for decision makers who are trying to forecast future events or the 
consequences of new policies.
a model with predictive precision may even be useful when it is empirically 
inaccurate. for instance, policymakers would value a structural model that predicts the 
timing of recessions, even if the model usually generated small predictive errors. an 
alternative model that correctly predicted that a recession would occur at some 
unspecified time over a ten year horizon would not be as useful. in general, models that 
make approximately accurate strong predictions are much more useful than models that 
make exactly accurate weak predictions. figure 2 provides a visual illustration of this 
point.
model = “x+y > 1” =
x
y
1
1
data =
panel a: model is falsifiable, empirically 
consistent, and does not have predictive 
precision. 
model = “(x,y) = (1,5)” =
data =
x
y
1
1
5
panel b: model is falsifiable, empirically 
inconsistent, and has predictive precision. 
figure 2:
falsifiability, empirical consistency, and predictive precision 
predictive precision is infrequently emphasized in economics research. academic 
economists have instead elevated properties like parsimony, tractability, conceptual 
insightfulness, and generalizability. we believe that this tendency arises because people
tend to celebrate the things they do best. economists have had a comparative advantage 
in developing elegant mathematical models. economists – including behavioral 
economists -- have been far less successful in developing general models that make 
precise quantitative predictions that are approximately empirically accurate. in this 
sense, economic research differs from research in the natural sciences, particularly 
physics. we hope that economists will close this gap. models that make weak 
predictions (or no predictions) are limited in their ability to advance economic
understanding of the world. 
is optimization a necessary ingredient for an economic model?
we have summarized seven key characteristics of a good model. some 
economists have argued that modeling criteria like those that we have discussed are 
incomplete. these economists formally define an economic model as a mathematical 
representation that has many of the features above and certain axiomatic optimization 
properties. for instance, rational beliefs and dynamic consistency have been proposed as 
axioms that define what it means to do economics. we do not agree with such a 
formulation.
we believe that economics is ultimately like any other science. scientific models 
are mathematical approximations of the world. in every other scientific field, these
approximations are judged by something akin to the seven criteria listed above and not by 
a model’s adoption of particular axiomatic assumptions.
in the history of science, every axiomatic litmus test has been discarded. in 
retrospect, it is easy to see that axioms like the flat earth, the earth at the center of the 
universe, or the euclidean structure of space, should not have been viewed as inviolate. it 
is likely that every axiom that we currently use in economics will suffer the same fate that 
these earlier axioms suffered. indeed, it is now believed that no branch of science will 
ever identify an inviolate set of axioms [kuhn 1962]. this does not mean that we should 
abandon axioms, only that we should not use axiomatic litmus tests to define a field of 
scientific inquiry.
relaxing economists’ commitment to optimization axioms poses several 
problems. optimization has provided discipline – i.e. parsimony. but optimization is 
neither necessary nor sufficient for parsimony. for example, physics models are not 
constrained by optimization and are nevertheless highly parsimonious. moreover, there 
are many optimization models that are not parsimonious since they make many special 
assumptions – about budget constraints and preferences -- to explain a single behavioral 
regularity.
without optimization axioms, economists will not able to rely on traditional 
normative tools like revealed preference. instead, economists must develop models in 
which true preferences interact with other factors -- like biases, errors, and dynamic 
inconsistency -- to produce economic behavior. economists will need to simultaneously 
model true preferences – hereafter called normative preferences -- and the confounding 
factors that prevent individuals from maximizing these normative preferences. such 
integrative models are not new. since the work of luce [1959], economists have been 
developing formal mathematical models that incorporate both normative preferences and 
decision-making errors.
4
to impute normative preferences, economists should adopt a two-step strategy 
that generalizes the classical revealed preference framework. first, specify a model that 
includes both normative preferences and a positive theory of behavior (incorporating 
factors like decision-making errors and/or dynamic inconsistency which force a wedge 
between revealed preferences and normative preferences). second, use choice data to 
estimate the model, thereby imputing the latent normative preferences.5
what is the appropriate scope of economic research?
all natural sciences have developed by incorporating the study of smaller and 
smaller units of analysis. biologists began with organisms, and then studied cells, cellular 
organs, and molecules (most importantly dna). physicists and their intellectual 
precursors began with visible objects and subsequently studied atoms and sub-atomic 
particles. similar intellectual progressions have also occurred in the social sciences. for 
instance, psychologists have recently embraced both neuroscience and molecular 
genetics. some anthropologists are also using these new methods.
we believe that economic research will also incorporate smaller units of analysis.
economics is usually described as the study of the allocation of scarce resources.
 
4
see bernheim and rangel [2006] – which is also published in this volume -- for an extended discussion of 
these issues.
5
see beshears et al [2006] for a more detailed discussion of these issues.
economists identify the individual decision-maker as the key actor in this resource 
allocation process. neuroscience6
and cognitive genetics7 will enable economists to 
develop a more complete understanding of the decision-making process. ultimately, 
such research will advance economists’ goal of predicting human behavior.
neuroscience and genetics data is not a necessary ingredient for economic research, but 
such biological data will speed up the economic research process by providing 
convergent types of evidence that complement traditional behavioral studies. these new 
data sources will enable economists to more quickly formulate, develop, and test models 
of decision-making and inter-individual variation. 
conclusion
this essay describes the seven key properties of successful economic models: 
parsimony, tractability, conceptual insightfulness, generalizability, falsifiability, 
empirical consistency, and predictive precision. however, even highly successful models 
do not have all seven properties. many of the properties are in conflict with one another.
for example, generalizing a model sometimes makes a model unfalsifiable -- the most 
general form of the theory of revealed preference can’t be rejected by behavioral data.
we encourage economists to give more weight to the property of predictive 
precision. models that make quantitatively precise predictions are the norm in other 
 
6
see camerer et al [2004] for a recent review.
7 ding et al [2006], benjamin et al [2007].
sciences. models with predictive precision are easy to empirically test and when such 
models are approximately empirically accurate they are likely to be useful.
bibliography
benjamin, daniel, j., christopher f. chabris, edward glaeser, vilmundur gudnason, 
tamara b. harris, david laibson, lenore launer, and shaun purcell. 2007. 
―genoeconomics.‖ forthcoming in [title to be announced, will enter it in  galleys], ed. maxine weinstein. national academy of sciences. 
bernheim, b. douglas and antonio rangel. 2006. ―title here,‖ forthcoming in [title of 
current volume], ed. andrew caplin and andrew schotter. 
beshears, john, james j. choi, david laibson, brigitte c. madrian. 2007. ―how are 
preferences revealed?‖ mimeo, harvard university.
camerer, colin, jonathan cohen, ernst fehr, paul glimcher, david laibson, george 
loewenstein, read montague. 2007. ―neuroeconomics.‖ forthcoming in the 
handbook of experimental economics, ed. john kagel and alvin roth. 
ding, weili, steven lehrer, j. niels rosenquist, and janet audrain-mcgovern. 2006.
the impact of poor health on education: new evidence using genetic markers. nber working paper 12304.
friedman. 1953. essays in positive economics. chicago: university of chicago press
jasso, guillermina. 2004. the tripartite structure of social science analysis.
sociological theory. 22:3, 401-431.
kuhn, thomas s. 1962. the structure of scientific revolutions. chicago: university of 
chicago press.
merton, robert k. 1973. the sociology of science. chicago: university of chicago 
press.
popper, karl r. 1934, republished in english 1959. the logic of scientific discovery. 
new york: basic books.
———. 1963. conjectures and refutations: the growth of scientific knowledge. new 
york: basic books.
stigler, george j. 1965. essays in the history of economics. chicago: the university of 
chicago press.
```query
"7 good properties of a good model"
```